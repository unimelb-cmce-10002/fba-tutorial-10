<!-- question-type: inclass -->
### Exercise 6: Comparing Outcomes Across Variations

We've now calculated completion rates for each variation.  
In this exercise, youâ€™ll quantify how much higher (or lower) the completion rate was in the Test variation compared to the Control group â€” and reflect on whether the difference is meaningful.

**(a)** Compute the **difference in percentage points** between the Test and Control variations.

```{r}
#| eval: false
completion_summary |> 
    summarise(
        diff_pct_points = YOUR_CODE
    )
```

**(b)** Compute the percent change in completion relative to the Control group.

```{r}
#| eval: false
completion_summary |> 
    mutate(
        YOUR_CODE
    )
```

**(c)**  You already have the difference in completion rates from part (a).  
Letâ€™s use that value directly and divide it by the standard deviation of completion in the Control group to see how large the effect is in practical terms.

```{r}
#| eval: false
# pull the numeric difference from part (a)
diff_rate <- completion_diff |> pull(diff_pct_points) / 100

# compute Control groupâ€™s SD
control_sd <- 
    completion_by_visitor |> 
    YOUR_CODE|> 
    YOUR_CODE(sd_rate = YOUR_CODE) |> 
    pull(sd_rate)

# difference in standard-deviation units of the control group
diff_sd_units <- diff_rate / control_sd
print(diff_sd_units)

```

**(d)** Would you consider this a small, moderate, or large effect in a typical online experiment? Explain Your answer.

<!-- BEGIN PROFILE:r-teaching-guide -->
::: {.content-visible when-profile="r-teaching-guide"}

::: {.teaching-block}

::: {.teaching-block-header}
Teaching Note
:::

::: {.teaching-block-body}

ğŸ¯ **Learning Objective**  
Students should:  
- Learn how to compute and interpret absolute (percentage-point) and relative (% change) differences between experimental groups.  
- Understand what it means to express an effect size in standard-deviation units.  
- Begin to reason about *practical significance* (is it large enough to matter?) rather than statistical significance.


âœ… **Core Concepts to Highlight**

- **Percentage points vs. percent change:**  
  Reinforce the distinction â€” 3.7 *percentage points* â‰  3.7 *percent increase*.  
  Percentage points measure **absolute change**, while percent change measures **relative improvement**.

- **Practical significance:**  
  Statistical tests arenâ€™t used here, but we can still ask: *Is the effect meaningful?*  
  A 3â€“4 p.p. lift might sound small but can be impactful at scale (e.g., in millions of sign-ups).

- **Standard deviation interpretation:**  
  Expressing differences in SD units (â‰ˆ 0.08) offers a benchmark for magnitude:  
  - 0.1 SD â‰ˆ small, 0.3 SD â‰ˆ moderate, 0.5 SD â‰ˆ large (rough guide).  
  This introduces the idea of *effect size* as a generalizable measure.

- **Binary outcome variance:**  
  Highlight that SD for completion = âˆš(p Ã— (1 âˆ’ p)) and why it matters when comparing scales of effects across contexts.

### What Counts as a Big Effect?

| Setting | Typical Range | Notes |
|----------|----------------|-------|
| **Online field experiments (A/B tests)** | 1â€“5 p.p. (â‰ˆ 0.02â€“0.10 SD) | Small by statistical standards but **commercially meaningful** at scale. |
| **Consumer-behaviour / lab experiments** | 5â€“20 p.p. (â‰ˆ 0.3â€“0.6 SD) | Larger effects due to **controlled environments** and more salient manipulations. |

**Why field effects look smaller:**
- High environmental noise  
- Real users, real distractions  
- Behavioural (not attitudinal) outcomes  

**Why lab effects look larger:**
- Controlled setting, short-term tasks  
- Lower variance and stronger stimuli  

> A 3.7 p.p. lift (â‰ˆ 0.08 SD) is **typical for a good online experiment** â€” modest statistically, substantial operationally.

ğŸ’¬ **Suggested In-Class Prompts**

- â€œWhy do we prefer to talk about *percentage points* instead of just percentages here?â€  
- â€œDoes a 3.7 p.p. difference sound small or large to you? What if this site has a million monthly visitors?â€  
- â€œWhatâ€™s the intuition behind dividing by the standard deviation?â€  
- â€œCan you think of a context where a small statistical effect might have a large business impact?â€

ğŸ“Œ **Common Misunderstandings**

- Confusing â€œpercent changeâ€ and â€œpercentage points.â€  
- Assuming that a small SD effect automatically means â€œnot important.â€  
- Treating a descriptive difference as proof of causality (thatâ€™s the next step).  
- Thinking that SD units are only for continuous variables â€” they apply here because `completed` is coded 0/1.

ğŸ§  **Instructor Notes**

- Emphasize *scale and impact*: in digital experiments, small average effects can compound massively.  
- Keep computation light â€” the conceptual takeaway is what matters.  
- Encourage discussion about what counts as a â€œbig effectâ€ in different industries (marketing, fintech, health).

:::

:::

:::
<!-- END PROFILE:r-teaching-guide -->

<!-- BEGIN PROFILE:r-solutions -->
::: {.content-visible when-profile="r-solutions" when-profile="r-teaching-guide"}

::: {.solution-block}

::: {.solution-block-header}
Solution
:::

::: {.solution-block-body}

**(a)** 
```{r}
completion_diff <- 
    completion_summary |> 
    summarise(
        diff_pct_points = diff(pct)
    )

completion_diff
```

`diff(pct)` subtracts the first value (Control) from the second (Test).

Example output: 3.7, meaning the Test groupâ€™s completion rate is 3.7 percentage points higher.

This gives an absolute difference in the same units as the rates (percentage points, not percent).

**(b)**
```{r}
completion_summary |> 
    mutate(
        pct_change = (pct - first(pct)) / first(pct) * 100
    )
```

Calculates the relative increase over the Control baseline.

Example:

(69.3âˆ’65.6)/65.6Ã—100â‰ˆ5.6%

Interpretation: The Test variation improved completions by roughly 5.6 percent relative to baseline.

**(c)** 
```{r}
# pull the numeric difference from part (a)
diff_rate <- completion_diff |> pull(diff_pct_points) / 100

# compute Control groupâ€™s SD
control_sd <- 
    completion_by_visitor |> 
    filter(variation == "Control") |> 
    summarise(sd_rate = sd(completed)) |> 
    pull(sd_rate)

# difference in SD units
diff_sd_units <- diff_rate / control_sd
print(diff_sd_units)
```

**(d)**
The Test variation increased completions by 3.7 percentage points, or about 5.6 percent relative to baseline.

In standard-deviation terms (~0.08 SDs), the effect is typical for A/B tests in digital environments, where even modest improvements can be valuable at scale.

:::

:::

:::
<!-- END PROFILE:r-solutions -->