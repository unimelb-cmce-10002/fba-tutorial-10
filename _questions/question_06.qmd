<!-- question-type: inclass -->
### Exercise 6: Comparing Outcomes Across Variations

We've now calculated completion rates for each variation.  
In this exercise, you’ll quantify how much higher (or lower) the completion rate was in the Test variation compared to the Control group — and reflect on whether the difference is meaningful.

**(a)** Compute the **difference in percentage points** between the Test and Control variations.

```{r}
#| eval: false
completion_summary |> 
    summarise(
        diff_pct_points = YOUR_CODE
    )
```

**(b)** Compute the percent change in completion relative to the Control group.

```{r}
#| eval: false
completion_summary |> 
    mutate(
        YOUR_CODE
    )
```

**(c)**  You already have the difference in completion rates from part (a).  
Let’s use that value directly and divide it by the standard deviation of completion in the Control group to see how large the effect is in practical terms.

```{r}
#| eval: false
# pull the numeric difference from part (a)
diff_rate <- completion_diff |> pull(diff_pct_points) / 100

# compute Control group’s SD
control_sd <- 
    completion_by_visitor |> 
    YOUR_CODE|> 
    YOUR_CODE(sd_rate = YOUR_CODE) |> 
    pull(sd_rate)

# difference in standard-deviation units of the control group
diff_sd_units <- diff_rate / control_sd
print(diff_sd_units)

```

**(d)** Would you consider this a small, moderate, or large effect in a typical online experiment? Explain Your answer.

<!-- BEGIN PROFILE:r-teaching-guide -->
::: {.content-visible when-profile="r-teaching-guide"}

::: {.teaching-block}

::: {.teaching-block-header}
Teaching Note
:::

::: {.teaching-block-body}

🎯 **Learning Objective**  
Students should:  
- Learn how to compute and interpret absolute (percentage-point) and relative (% change) differences between experimental groups.  
- Understand what it means to express an effect size in standard-deviation units.  
- Begin to reason about *practical significance* (is it large enough to matter?) rather than statistical significance.


✅ **Core Concepts to Highlight**

- **Percentage points vs. percent change:**  
  Reinforce the distinction — 3.7 *percentage points* ≠ 3.7 *percent increase*.  
  Percentage points measure **absolute change**, while percent change measures **relative improvement**.

- **Practical significance:**  
  Statistical tests aren’t used here, but we can still ask: *Is the effect meaningful?*  
  A 3–4 p.p. lift might sound small but can be impactful at scale (e.g., in millions of sign-ups).

- **Standard deviation interpretation:**  
  Expressing differences in SD units (≈ 0.08) offers a benchmark for magnitude:  
  - 0.1 SD ≈ small, 0.3 SD ≈ moderate, 0.5 SD ≈ large (rough guide).  
  This introduces the idea of *effect size* as a generalizable measure.

- **Binary outcome variance:**  
  Highlight that SD for completion = √(p × (1 − p)) and why it matters when comparing scales of effects across contexts.

### What Counts as a Big Effect?

| Setting | Typical Range | Notes |
|----------|----------------|-------|
| **Online field experiments (A/B tests)** | 1–5 p.p. (≈ 0.02–0.10 SD) | Small by statistical standards but **commercially meaningful** at scale. |
| **Consumer-behaviour / lab experiments** | 5–20 p.p. (≈ 0.3–0.6 SD) | Larger effects due to **controlled environments** and more salient manipulations. |

**Why field effects look smaller:**
- High environmental noise  
- Real users, real distractions  
- Behavioural (not attitudinal) outcomes  

**Why lab effects look larger:**
- Controlled setting, short-term tasks  
- Lower variance and stronger stimuli  

> A 3.7 p.p. lift (≈ 0.08 SD) is **typical for a good online experiment** — modest statistically, substantial operationally.

💬 **Suggested In-Class Prompts**

- “Why do we prefer to talk about *percentage points* instead of just percentages here?”  
- “Does a 3.7 p.p. difference sound small or large to you? What if this site has a million monthly visitors?”  
- “What’s the intuition behind dividing by the standard deviation?”  
- “Can you think of a context where a small statistical effect might have a large business impact?”

📌 **Common Misunderstandings**

- Confusing “percent change” and “percentage points.”  
- Assuming that a small SD effect automatically means “not important.”  
- Treating a descriptive difference as proof of causality (that’s the next step).  
- Thinking that SD units are only for continuous variables — they apply here because `completed` is coded 0/1.

🧠 **Instructor Notes**

- Emphasize *scale and impact*: in digital experiments, small average effects can compound massively.  
- Keep computation light — the conceptual takeaway is what matters.  
- Encourage discussion about what counts as a “big effect” in different industries (marketing, fintech, health).

:::

:::

:::
<!-- END PROFILE:r-teaching-guide -->

<!-- BEGIN PROFILE:r-solutions -->
::: {.content-visible when-profile="r-solutions" when-profile="r-teaching-guide"}

::: {.solution-block}

::: {.solution-block-header}
Solution
:::

::: {.solution-block-body}

**(a)** 
```{r}
completion_diff <- 
    completion_summary |> 
    summarise(
        diff_pct_points = diff(pct)
    )

completion_diff
```

`diff(pct)` subtracts the first value (Control) from the second (Test).

Example output: 3.7, meaning the Test group’s completion rate is 3.7 percentage points higher.

This gives an absolute difference in the same units as the rates (percentage points, not percent).

**(b)**
```{r}
completion_summary |> 
    mutate(
        pct_change = (pct - first(pct)) / first(pct) * 100
    )
```

Calculates the relative increase over the Control baseline.

Example:

(69.3−65.6)/65.6×100≈5.6%

Interpretation: The Test variation improved completions by roughly 5.6 percent relative to baseline.

**(c)** 
```{r}
# pull the numeric difference from part (a)
diff_rate <- completion_diff |> pull(diff_pct_points) / 100

# compute Control group’s SD
control_sd <- 
    completion_by_visitor |> 
    filter(variation == "Control") |> 
    summarise(sd_rate = sd(completed)) |> 
    pull(sd_rate)

# difference in SD units
diff_sd_units <- diff_rate / control_sd
print(diff_sd_units)
```

**(d)**
The Test variation increased completions by 3.7 percentage points, or about 5.6 percent relative to baseline.

In standard-deviation terms (~0.08 SDs), the effect is typical for A/B tests in digital environments, where even modest improvements can be valuable at scale.

:::

:::

:::
<!-- END PROFILE:r-solutions -->