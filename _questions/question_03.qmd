<!-- question-type: prepare -->
### Exercise 3: Exploring the Experimental Data

It‚Äôs time to look at the data from the experiment.

You have two datasets:  
- `ab_test_demographics`: information about each client.  
- `ab_test_experiment_clients`: information on which clients were assigned to the **Control** or **Test** variation.

Our goal is to merge these datasets and check whether allocation to treatment and control groups is balanced across deomgraphic characteristics.

**(a)** Load the two datasets into R by completing the code below:

```{r}
#| eval: false
demogs <- YOURCODE 
treatment <- 
    YOURCODE |> 
    clean_names()
```

**(b)** Merge the treatment indicator variable into the `demogs` data by completing the code below. We recommend you use an `inner_join()` to do this.

```{r}
#| eval: false
demogs <- 
    demogs |>
    YOURCODE

```

**(c)** What are the unique values in `variation` column? If needed, drop any rows of data that you won't be able to use in the Treatment vs Control comparison.

```{r}
#| eval: false
# how values in variation?
demogs |> 
    YOURCODE

# Perform any required operations to remove rows
# from the data
demogs <-
    demogs |> 
    YOURCODE
```

**(d)** In a well-designed experiment, each variation should include comparable types of users. How does connect back to the idea of random assignment?

**(e)** Complete the code below to compare demographic compositions of users across treatments. Explain your findings.

```{r}
#| eval: false
demogs |> 
    select(-client_id) |> 
    st(group = 'YOUR_TREATMENT_INDICATOR')
```


**(f)** Why is checking balance between treatments the first thing we do before comparing performance?

<!-- BEGIN PROFILE:r-teaching-guide -->
::: {.content-visible when-profile="r-teaching-guide"}

::: {.teaching-block}

::: {.teaching-block-header}
Teaching Note
:::

::: {.teaching-block-body}

üéØ **Learning Objective**  
Students should:  

- Learn how to combine demographic and experimental‚Äêassignment data to prepare for analysis.  
- Understand why checking group balance is essential for credible experimental inference.  
- Recognize how random assignment supports fair comparison between treatment and control groups.  
- Connect data‚Äêcleaning steps (joins, missing values) to experimental design logic.

‚úÖ **Core Concepts to Highlight**

- **Data integration:**  
  Experiments often require merging multiple datasets (user characteristics + treatment allocation).  
  Clean joins are critical for ensuring every observation has complete information.

- **Uniqueness of identifiers:**  
  Each client ID should appear only once per dataset; duplicates threaten the logic of random assignment.

- **Random assignment and balance:**  
  If assignment worked, demographic characteristics should be evenly distributed across variations.  
  Balance gives confidence that outcome differences later reflect the experimental change.

- **Interpreting balance tables:**  
  Nearly identical means or proportions across groups indicate success.  
  Minor random variation is expected; systematic gaps signal implementation issues.

- **Why balance matters first:**  
  Establishing comparability before outcome analysis preserves causal credibility.

üí¨ **Suggested In-Class Prompts**

- ‚ÄúWhat could happen if one group had systematically older or more experienced users?‚Äù
- ‚ÄúWhy might a random process still produce small imbalances?‚Äù  
- ‚ÄúHow would you explain to a manager why we check balance before looking at results?‚Äù
- [Advanced] "How could we test whether there are differences between treatments?"
- ‚ÄúIf random assignment failed, can we still interpret completion differences as causal?‚Äù

üìå **Common Misunderstandings**

- Expecting perfect (identical) balance rather than approximate similarity.  
- Thinking balance checks are only about fairness rather than validity of inference.  


:::

:::

:::
<!-- END PROFILE:r-teaching-guide -->

<!-- BEGIN PROFILE:r-solutions -->
::: {.content-visible when-profile="r-solutions" when-profile="r-teaching-guide"}

::: {.solution-block}

::: {.solution-block-header}
Solution
:::

::: {.solution-block-body}

**(a)**  
Load both datasets and inspect them.

```{r}
demogs <- 
    read_csv("data/ab_test_demographics.txt")
treatment <- 
    read_csv("data/ab_test_experiment_clients.txt") |> 
    janitor::clean_names()
```

**(b)** 

```{r}
demogs <- 
    demogs |>
    inner_join(treatment, join_by(client_id))
```

**(c)**

```{r}
# view unique values
demogs |> 
    distinct(variation)

# drop missing or unusable rows
demogs <- 
    demogs |> 
    drop_na(variation)
```

**(d)** 
In a well-designed experiment, both variations should contain similar types of users.
This connects directly to the idea of random assignment ‚Äî if users are randomly allocated, differences in demographics should appear only by chance.
Balanced characteristics increase confidence that later performance differences are due to the new sign-up flow, not underlying user differences.

**(e)** 
Compare demographic balance across treatments.

```{r}
demogs |> 
    select(-client_id) |> 
    st(group = 'variation')
```

The summary table (`st()`) should show nearly identical means and proportions across groups.

Small random differences are expected, but no systematic gaps should appear.

**(f)**
Checking balance is the first step before comparing performance because:

- It verifies that random assignment was implemented correctly.
- If groups differ systematically at baseline, outcome differences could reflect pre-existing characteristics rather than the experimental change.
- Confirming balance ensures that any observed effects can be credibly attributed to the new sign-up flow.
:::

:::

:::
<!-- END PROFILE:r-solutions -->